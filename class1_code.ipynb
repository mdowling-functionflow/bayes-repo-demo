{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "introduction_churn"
   },
   "source": [
    "# Applied Machine Learning - Class 1\n",
    "\n",
    "Welcome to the first class of Applied Machine Learning! In this session, we will get familiar with loading data, cleaning it, visualizing, and building predictive models. We'll use a **Telco Customer Churn dataset** to predict whether a customer is likely to stop using a company's services.\n",
    "\n",
    "**Business Use Case:** Customer churn (or attribution) is a major concern for many businesses, especially those with subscription models (telecom, streaming, SaaS, etc.). Losing customers means losing revenue. Identifying customers at risk of churning allows businesses to take proactive steps to retain them, such as offering special promotions or addressing their concerns. Our goal is to classify customers as 'Will Not Churn' (0) or 'Will Churn' (1).\n",
    "\n",
    "Let's get started! üìâ‚û°Ô∏èüìà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports_churn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Load the Telco Customer Churn dataset\n",
    "# Make sure 'class1_dataset.csv' is loaded in the sidebar\n",
    "try:\n",
    "    df = pd.read_csv('class1_dataset.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'class1_dataset.csv' not found. Please download it and place it in the correct directory.\")\n",
    "    # df = pd.DataFrame() # Placeholder to allow notebook to run further cells, but they won't be meaningful\n",
    "    raise # Re-raise the error\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "basic_exploration_churn"
   },
   "source": [
    "## Basic Data Exploration üìä\n",
    "\n",
    "Let's look at the structure of our customer data and check for any immediate issues like missing values or incorrect data types. The features describe customer demographics, account information, and services they use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_structure_churn"
   },
   "outputs": [],
   "source": [
    "# Check the structure of the dataset\n",
    "print(\"\\nDataset Information:\")\n",
    "df.info()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_cleaning_header_churn"
   },
   "source": [
    "### Data Cleaning and Initial Transformation\n",
    "Some columns might need adjustments:\n",
    "1.  `customerID`: This is just an identifier and won't be useful for modeling, so we can drop it.\n",
    "2.  `TotalCharges`: This column is sometimes read as an object type if it contains spaces (representing new customers with no charges yet). We need to convert it to a numeric type and handle any resulting missing values (e.g., by filling with 0 or median/mean for those few cases).\n",
    "3.  `Churn`: Our target variable is 'Yes'/'No'. We'll convert this to 1/0 for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_cleaning_churn"
   },
   "outputs": [],
   "source": [
    "# Drop customerID as it's not a predictive feature\n",
    "df = df.drop('customerID', axis=1)\n",
    "\n",
    "# Convert 'TotalCharges' to numeric. Errors='coerce' will turn non-numeric values (like spaces) into NaN\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Check for NaNs created in TotalCharges\n",
    "print(f\"\\nNaNs in TotalCharges after conversion: {df['TotalCharges'].isnull().sum()}\")\n",
    "\n",
    "# Handle missing TotalCharges. For new customers, TotalCharges might be 0. Or impute with median.\n",
    "# Let's see which customers have missing TotalCharges - often those with tenure 0\n",
    "# print(df[df['TotalCharges'].isnull()][['tenure', 'MonthlyCharges', 'TotalCharges']])\n",
    "df['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].median()) # Impute with median\n",
    "\n",
    "# Convert target variable 'Churn' to binary (0/1)\n",
    "df['Churn'] = df['Churn'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "print(\"\\nCleaned dataset info:\")\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "descriptive_stats_churn"
   },
   "source": [
    "## Descriptive Statistics\n",
    "\n",
    "Summary statistics for numerical features help us understand their distributions (e.g., average tenure, monthly charges)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "summary_stats_churn"
   },
   "outputs": [],
   "source": [
    "# Get summary statistics for numerical columns\n",
    "print(\"\\nDescriptive Statistics (Numerical Features):\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualizations_churn_header"
   },
   "source": [
    "## Data Visualizations: Understanding Customer Behavior\n",
    "\n",
    "Visualizing data helps identify patterns related to churn. We'll look at:\n",
    "* The overall churn rate.\n",
    "* How churn varies with features like contract type, tenure, and monthly charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "churn_distribution_plot_churn"
   },
   "outputs": [],
   "source": [
    "# Plot a pie chart for churn distribution\n",
    "churn_counts = df['Churn'].value_counts()\n",
    "fig_churn_pie = px.pie(values=churn_counts.values,\n",
    "                       names=churn_counts.index.map({0: 'No Churn', 1: 'Churn'}),\n",
    "                       title='Customer Churn Distribution',\n",
    "                       hole=0.3)\n",
    "fig_churn_pie.update_traces(textinfo='percent+label')\n",
    "fig_churn_pie.show()\n",
    "print(f\"Customers who did not churn: {churn_counts.get(0, 0)}\")\n",
    "print(f\"Customers who churned: {churn_counts.get(1, 0)}\")\n",
    "print(f\"Churn rate: {churn_counts.get(1, 0) / len(df) * 100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "more_visualizations_churn"
   },
   "outputs": [],
   "source": [
    "# Example: Churn by Contract type\n",
    "fig_contract_churn = px.histogram(df, x='Contract', color='Churn',\n",
    "                                  barmode='group', text_auto=True,\n",
    "                                  title='Churn Distribution by Contract Type',\n",
    "                                  labels={'Churn': 'Churn Status (0: No, 1: Yes)'})\n",
    "fig_contract_churn.show()\n",
    "\n",
    "# Example: Tenure distribution by Churn\n",
    "fig_tenure_churn = px.histogram(df, x='tenure', color='Churn', marginal='box',\n",
    "                                title='Tenure Distribution by Churn Status',\n",
    "                                labels={'Churn': 'Churn Status (0: No, 1: Yes)'})\n",
    "fig_tenure_churn.show()\n",
    "\n",
    "# Example: MonthlyCharges distribution by Churn\n",
    "fig_monthly_churn = px.histogram(df, x='MonthlyCharges', color='Churn', marginal='box',\n",
    "                                 title='Monthly Charges Distribution by Churn Status',\n",
    "                                 labels={'Churn': 'Churn Status (0: No, 1: Yes)'})\n",
    "fig_monthly_churn.show()\n",
    "\n",
    "print(\"Visualizations like these help understand factors correlated with churn. For example, customers on month-to-month contracts tend to churn more, and customers with very low or very high tenure might show different churn behaviors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_prep_churn_header"
   },
   "source": [
    "## Preparing the Data for Modeling üõ†Ô∏è\n",
    "\n",
    "Machine learning models require numerical input. We need to:\n",
    "1.  **Identify Categorical and Numerical Features:** Separate columns by data type.\n",
    "2.  **One-Hot Encode Categorical Features:** Convert categorical variables into a numerical format that models can understand (e.g., 'Contract' type 'Month-to-month' becomes a set of binary columns).\n",
    "3.  **Scale Numerical Features:** Bring numerical features (like 'tenure', 'MonthlyCharges') to a similar scale using `StandardScaler`. This helps algorithms that are sensitive to feature magnitudes (e.g., Logistic Regression, SVC, Neural Networks).\n",
    "4.  **Define Features (X) and Target (y):** 'Churn' is our target.\n",
    "5.  **Split Data:** Divide into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feature_encoding_scaling_churn"
   },
   "outputs": [],
   "source": [
    "# Define features (X) and target variable (y)\n",
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "print(f\"Categorical features: {list(categorical_features)}\")\n",
    "print(f\"Numerical features: {list(numerical_features)}\")\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical features\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore', drop='first') # drop='first' to avoid multicollinearity\n",
    "\n",
    "# Create a column transformer to apply transformations to the correct columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough' # In case there are columns not specified (shouldn't be here)\n",
    ")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# We use stratify by 'y' to ensure similar class proportions in train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply preprocessing: fit on training data, transform both training and testing data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Get feature names after one-hot encoding for better interpretability later (optional but good)\n",
    "try:\n",
    "    feature_names_out = preprocessor.get_feature_names_out()\n",
    "except AttributeError:\n",
    "    # For older scikit-learn versions, a bit more manual work might be needed if you want exact names\n",
    "    # For now, we'll proceed without them for simplicity if get_feature_names_out is not available\n",
    "    feature_names_out = None\n",
    "    print(\"Note: preprocessor.get_feature_names_out() not available. Using processed data without explicit new feature names.\")\n",
    "\n",
    "print(\"\\nX_train_processed shape:\", X_train_processed.shape)\n",
    "print(\"X_test_processed shape:\", X_test_processed.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# If you used a sparse matrix from OneHotEncoder and want to convert to dense for some models/operations:\n",
    "if hasattr(X_train_processed, \"toarray\"):\n",
    "    X_train_processed = X_train_processed.toarray()\n",
    "    X_test_processed = X_test_processed.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_intro_churn"
   },
   "source": [
    "## Machine Learning Models for Churn Prediction\n",
    "\n",
    "We will now train several classification models to predict customer churn. For each model, we will:\n",
    "1.  Train the model on the processed `X_train_processed` and `y_train` data.\n",
    "2.  Make predictions on the processed `X_test_processed` data.\n",
    "3.  Evaluate its performance using:\n",
    "    * **Accuracy:** Overall correctness of predictions.\n",
    "    * **Confusion Matrix:** A table showing True Positives (correctly predicted churn), True Negatives (correctly predicted no churn), False Positives (predicted churn, but didn't), and False Negatives (predicted no churn, but did).\n",
    "\n",
    "For this introductory class, we are focusing on accuracy and the confusion matrix. In more advanced sessions, you'd explore other metrics like precision, recall, F1-score, and AUC, especially when dealing with datasets where one class is much rarer than the other (imbalanced datasets) or when the costs of different types of errors vary significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "logistic_regression_churn_header"
   },
   "source": [
    "### 1. Logistic Regression\n",
    "\n",
    "A good baseline linear model for binary classification. It estimates the probability of a customer churning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "logistic_regression_churn_code"
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42, solver='liblinear') # Specified solver for potentially better convergence\n",
    "lr.fit(X_train_processed, y_train)\n",
    "y_pred_lr = lr.predict(X_test_processed)\n",
    "\n",
    "print(\"--- Logistic Regression --- \")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr) * 100:.2f}%\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "fig_lr = ff.create_annotated_heatmap(z=cm_lr, x=['No Churn', 'Churn'], y=['No Churn', 'Churn'], colorscale='Viridis')\n",
    "fig_lr.update_layout(title='Confusion Matrix for Logistic Regression (Actual vs. Predicted)',\n",
    "                     xaxis_title=\"Predicted Label\", yaxis_title=\"Actual Label\")\n",
    "fig_lr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cm_interpretation_churn"
   },
   "source": [
    "**Interpreting the Confusion Matrix for Churn:**\n",
    "```\n",
    "                Predicted\n",
    "                No Churn    Churn\n",
    "Actual  No Churn     TN          FP  (Type I Error - Predicted churn, but customer stayed)\n",
    "        Churn        FN          TP  (Type II Error - Predicted no churn, but customer left)\n",
    "```\n",
    "* **True Negatives (TN):** Customers correctly identified as *not* churning.\n",
    "* **False Positives (FP):** Customers predicted to churn, but they actually stayed. (Cost: unnecessary retention efforts).\n",
    "* **False Negatives (FN):** Customers predicted to stay, but they actually churned. (Cost: lost customer and revenue, often the primary concern).\n",
    "* **True Positives (TP):** Customers correctly identified as churning.\n",
    "\n",
    "The ideal scenario is to maximize TN and TP, while minimizing FP and FN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "decision_tree_churn_header"
   },
   "source": [
    "### 2. Decision Tree\n",
    "\n",
    "A non-linear model that creates a tree of rules. Can be very interpretable but might overfit without tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "decision_tree_churn_code"
   },
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(random_state=42, max_depth=5) # Added max_depth to prevent overfitting for illustration\n",
    "dt.fit(X_train_processed, y_train)\n",
    "y_pred_dt = dt.predict(X_test_processed)\n",
    "\n",
    "print(\"--- Decision Tree --- \")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_dt) * 100:.2f}%\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "fig_dt = ff.create_annotated_heatmap(z=cm_dt, x=['No Churn', 'Churn'], y=['No Churn', 'Churn'], colorscale='Blues')\n",
    "fig_dt.update_layout(title='Confusion Matrix for Decision Tree (Actual vs. Predicted)',\n",
    "                     xaxis_title=\"Predicted Label\", yaxis_title=\"Actual Label\")\n",
    "fig_dt.show()\n",
    "\n",
    "# Optional: Visualize the tree (might be large for many features)\n",
    "# from sklearn.tree import plot_tree\n",
    "# import matplotlib.pyplot as plt\n",
    "# if feature_names_out is not None:\n",
    "#   plt.figure(figsize=(20,10))\n",
    "#   plot_tree(dt, filled=True, feature_names=list(feature_names_out), class_names=['No Churn', 'Churn'], max_depth=3, fontsize=10)\n",
    "#   plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svc_churn_header"
   },
   "source": [
    "### 3. Support Vector Classifier (SVC)\n",
    "\n",
    "Finds a hyperplane that best separates the classes. Can be powerful but sometimes slower to train on very large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svc_churn_code"
   },
   "outputs": [],
   "source": [
    "# Support Vector Classifier\n",
    "# Using a linear kernel for faster training in this example. Non-linear kernels (e.g., 'rbf') can be more powerful but slower.\n",
    "svc = SVC(kernel='linear', random_state=42)\n",
    "print(\"Starting SVC training... This might take a moment.\")\n",
    "svc.fit(X_train_processed, y_train)\n",
    "print(\"SVC training complete.\")\n",
    "y_pred_svc = svc.predict(X_test_processed)\n",
    "\n",
    "print(\"--- Support Vector Classifier --- \")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_svc) * 100:.2f}%\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_svc = confusion_matrix(y_test, y_pred_svc)\n",
    "fig_svc = ff.create_annotated_heatmap(z=cm_svc, x=['No Churn', 'Churn'], y=['No Churn', 'Churn'], colorscale='Greens')\n",
    "fig_svc.update_layout(title='Confusion Matrix for SVC (Actual vs. Predicted)',\n",
    "                     xaxis_title=\"Predicted Label\", yaxis_title=\"Actual Label\")\n",
    "fig_svc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rf_churn_header"
   },
   "source": [
    "### 4. Random Forest\n",
    "\n",
    "An ensemble model using multiple decision trees. Generally robust and performs well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rf_churn_code"
   },
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, max_depth=10) # n_jobs=-1 uses all processors, added max_depth\n",
    "rf.fit(X_train_processed, y_train)\n",
    "y_pred_rf = rf.predict(X_test_processed)\n",
    "\n",
    "print(\"--- Random Forest --- \")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf) * 100:.2f}%\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "fig_rf = ff.create_annotated_heatmap(z=cm_rf, x=['No Churn', 'Churn'], y=['No Churn', 'Churn'], colorscale='Oranges')\n",
    "fig_rf.update_layout(title='Confusion Matrix for Random Forest (Actual vs. Predicted)',\n",
    "                     xaxis_title=\"Predicted Label\", yaxis_title=\"Actual Label\")\n",
    "fig_rf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gb_churn_header"
   },
   "source": [
    "### 5. Gradient Boosting\n",
    "\n",
    "Another powerful ensemble technique that builds trees sequentially, each correcting its predecessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gb_churn_code"
   },
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "gb = GradientBoostingClassifier(random_state=42, n_estimators=100, max_depth=3) # Added max_depth\n",
    "gb.fit(X_train_processed, y_train)\n",
    "y_pred_gb = gb.predict(X_test_processed)\n",
    "\n",
    "print(\"--- Gradient Boosting --- \")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_gb) * 100:.2f}%\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_gb = confusion_matrix(y_test, y_pred_gb)\n",
    "fig_gb = ff.create_annotated_heatmap(z=cm_gb, x=['No Churn', 'Churn'], y=['No Churn', 'Churn'], colorscale='Purples')\n",
    "fig_gb.update_layout(title='Confusion Matrix for Gradient Boosting (Actual vs. Predicted)',\n",
    "                     xaxis_title=\"Predicted Label\", yaxis_title=\"Actual Label\")\n",
    "fig_gb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlp_churn_header"
   },
   "source": [
    "### 6. Multi-layer Perceptron (MLP) - Neural Network\n",
    "\n",
    "A basic neural network. Can model complex relationships but may require more data and tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlp_churn_code"
   },
   "outputs": [],
   "source": [
    "# Multi-layer Perceptron Classifier\n",
    "# Scaled data is important for neural networks.\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=300, random_state=42, early_stopping=True, n_iter_no_change=10) # Simplified architecture, added early stopping\n",
    "mlp.fit(X_train_processed, y_train)\n",
    "y_pred_mlp = mlp.predict(X_test_processed)\n",
    "\n",
    "print(\"--- Multi-layer Perceptron (MLP) --- \")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_mlp) * 100:.2f}%\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_mlp = confusion_matrix(y_test, y_pred_mlp)\n",
    "fig_mlp = ff.create_annotated_heatmap(z=cm_mlp, x=['No Churn', 'Churn'], y=['No Churn', 'Churn'], colorscale='Greys')\n",
    "fig_mlp.update_layout(title='Confusion Matrix for MLP (Actual vs. Predicted)',\n",
    "                     xaxis_title=\"Predicted Label\", yaxis_title=\"Actual Label\")\n",
    "fig_mlp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion_churn"
   },
   "source": [
    "## Conclusion & Next Steps üöÄ\n",
    "\n",
    "In this class, we've explored a common business problem ‚Äì customer churn ‚Äì and applied various machine learning models to predict it. We covered:\n",
    "* Loading and **cleaning real-world data**, including handling missing values and incorrect data types (like 'TotalCharges').\n",
    "* The importance of **feature preprocessing**: converting categorical features to a numerical format (one-hot encoding) and scaling numerical features.\n",
    "* Visualizing data to understand churn patterns.\n",
    "* Training several different classification models, from simple linear models to more complex ensembles and neural networks.\n",
    "* Evaluating models using **accuracy** and interpreting the **confusion matrix** to understand the types of errors our models make.\n",
    "\n",
    "**Further Exploration:**\n",
    "* **Feature Importance:** Understanding which factors are most predictive of churn (e.g., contract type, tenure). Tree-based models like Random Forest and Gradient Boosting can provide feature importance scores.\n",
    "* **Cost-Benefit Analysis:** In a real business scenario, the cost of a False Negative (failing to predict a churner) is usually higher than a False Positive (incorrectly flagging a loyal customer for a retention offer). This can lead to choosing models or thresholds that specifically minimize high-cost errors.\n",
    "* **Advanced Evaluation Metrics:** Exploring precision, recall, F1-score, and ROC-AUC to get a better sense of model performance, especially if the churn rate was very low (imbalanced).\n",
    "* **Hyperparameter Tuning:** Optimizing the settings of each model (e.g., number of trees in a Random Forest, layers in an MLP) to improve performance.\n",
    "* **Interpretable AI (XAI):** Using techniques like SHAP or LIME to explain individual predictions, which is crucial for business adoption and trust.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
